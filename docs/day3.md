# Day 3

## Experiment Tracking with ML Flow

[ML Flow](https://mlflow.org/) is a popular open-source platform to manage ML lifecycle, including experiment tracking and a central model registry.

### Concepts

**Experiments**
ML Flow is organized by experiments. Each experiment is typically made of multiple runs of training job with possibly different parameters or data.

**Run**
Each run is one execution of a trainining job. A run can log the model parameters, metrics and artifacts.

**Artifacts**

Artifacts are the files that we would like to attach to a training run. These are typically the generated by the training, like a model file or an evaluation report.

**Model Registry**
Apart from storing model as part of each training run, mlflow also provides an option to add a model to a central model registry and make it possible to search the available models in the model registry.

### Getting Started

Install `mlflow` python package to get started.

```
$ pip install mlflow
```

For this training, we've setup a ML flow server install for all of you to use at <https://mlflow.pipal.in/>. You can open that web site and look at the existing experiments and runs.

Let's try a sample run. You could try this in your notebook.

```
import mlflow

mlflow.set_tracking_uri("https://mlflow.pipal.in/")

# You can pick a different experiment name
mlflow.set_experiment("hello-world-anand")

with mlflow.start_run():
    # log a param
    mlflow.log_param("param1", 10)

    # log a metric
    mlflow.log_metric("accuracy", 99.2)

    # log an artifact
    with open("hello.txt", "w") as f:
        f.write("hello, world!")
    mlflow.log_artifact("hello.txt")

print("done")
```

Once you run this code, you'll see a new experiment and a run recorded for it. If you run the code again, you'll see one more run under the same experiment. What ever params, metrics and artifacts that we logged will be shown with the run.

### Training a scikit-learn model

Mlflow supports autologging of many popular ML libraries. We just need to make a function call to enable autologging.

```
import pandas as pd
import mlflow
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

mlflow.set_tracking_uri("https://mlflow.pipal.in/")
mlflow.set_experiment("Penguins Classification")
mlflow.autolog()

print("reading the data...")
url = "https://raw.githubusercontent.com/amitkaps/mlops/main/datasets/palmerpenguins/penguins.csv"
df = pd.read_csv(url)

print("preparing the data...")
df = df.dropna()

columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
X = df[columns]
y = df.species

print("splitting the data into train and test...")
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

with mlflow.start_run():
    print("training the model...")
    params = dict(random_state=44, max_depth=6)
    model = DecisionTreeClassifier(**params)
    model.fit(X_train, y_train)

    mlflow.sklearn.log_model(
        sk_model=model,
        artifact_path="model",
        registered_model_name="penguins-classfier"
    )
    print("validating...")
    y_pred = model.predict(X_test)
    test_accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)
    print("Accuracy on test data:", test_accuracy)
```