# **Practical ML Operations (MLOps)**

{% callout type="note" %}
This is work in progress.
{% /callout %}

## _Scope & Session Plan_

- _Day 1:_ MLOps Landscape and Building Reproducible Prototype
- _Day 2:_ Continuous Training - Data, Model, Experiments & Testing
- _Day 3:_ Continuous Deployment - Serving, Inference, Feedback & System Performance

---

### Session 1: **Introduction & Concepts**

- Challenges for ML driven Operations in Production
- Requirements: Reliable, Scalable, Maintainable, Adaptable
- Considerations: Framing, Objectives, Constraints & Phases
- Thinking in Systems: Application, Business and Infrastructure

### Session 2: **Systems & Operations**

- Current Landscape and Maturity Level for ML Ops
- Define ML Ops: Requirements, Considerations, Interfaces, Data & Model
- Understand Process, Skills, and Tooling to move across Maturity Levels
- View Case Studies and Best Practices of ML Ops from the Industry

### Session 3: **Prototype & Modularize**

- Build a basic task-focused ML model for the case problem
- Create, build and deploy a local serving prototype ML system
- Modularize the Prototype Code, Data and Model for improved iteration
- Understand Trade-offs and Benefits of Modularized Approach

### Session 4: **Versioning & Components**

- Versioning in ML Ops: Workflow & Components
- Declarative Code, Data & Model Design Patterns
- Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts
- Create a Versioned System for the Case Example

### Session 5: **Organising & Preparation**

- Manage Data in ML Ops: Organize, Prepare & Reproduce
- Training Data Prep: Extraction, Sampling, Splitting & Windowing
- Component Setup: Labelling, Preprocessing, & Augmentation
- Modularise & isolate components for data preparation

### Session 6: **Experiments & Training**

- Baselines: Heuristic-driven, Simple ML baselines
- Setup for Rapid Experiments: Repeatable and Reproducible
- Experiment Tracking: Model Parameters, Optimisation
- Organise & Package for Data, Compute, and Infrastructure needs

### Session 7: **Evaluation & Metrics**

- Coarse-grained vs Fine-grained Model Evaluation
- Evaluation for Robustness: Stratification & Slicing, Confidence
- Metrics vs. Business Objectives & Interpretations
- Tracking evaluation metrics for coverage and failure cases

### Session 8: **Testing & Validation**

- Code Testing: Unit, Integration, System & Acceptance
- Data Testing: Schema, Expectations, Quality, Distribution Skew
- Model Testing: Training, Evaluation, Inference & Deployment
- Behavioural Testing: Invariance, Functionality, Directionality

### Session 9: **Serving & Inference**

- Align Objectives and Interfaces for Serving e.g Latency
- Inference Optimisation: Accelerators, Model Quantisation / Distillation
- Design for Production: Packages, Executables and Artefacts
- Thinking in Stages: Test, Canary, Development, Production

### Session 10: **Monitoring & Feedback**

- Performance monitoring and Post-Hoc Evaluation
- Predictive monitoring: Delayed Outcomes, Importance Weighting
- Measuring & Locating Drift: Data (Label & Feature), Concept
- Feedback loop: Integrating with Application Logging and Monitoring

### Session 11: **Levels & Practice**

- Advanced Levels of Maturity for MLOps
- Dataset Management: Catalogs, Quality, Lineage
- Feature Stores and Pipeline Orchestration
- Continuous Monitoring and Automated Triggering

### Session 12: **Best Practices**

- Advanced Case Study of a Successful MLOps
- Review of Best Practices for MLOps in Production
- Moving towards Continual Learning Data-centric Systems
- Learning Path and Way Forward
