{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Practical ML Operations (MLOps) !!! note This is work in progress. Scope & Session Plan Day 1: MLOps Landscape and Building Reproducible Prototype Day 2: Continuous Training - Data, Model, Experiments & Testing Day 3: Continuous Deployment - Serving, Inference, Feedback & System Performance Session 1: Introduction & Concepts Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure Session 2: Systems & Operations Current Landscape and Maturity Level for ML Ops Define ML Ops: Requirements, Considerations, Interfaces, Data & Model Understand Process, Skills, and Tooling to move across Maturity Levels View Case Studies and Best Practices of ML Ops from the Industry Session 3: Prototype & Modularize Build a basic task-focused ML model for the case problem Create, build and deploy a local serving prototype ML system Modularize the Prototype Code, Data and Model for improved iteration Understand Trade-offs and Benefits of Modularized Approach Session 4: Versioning & Components Versioning in ML Ops: Workflow & Components Declarative Code, Data & Model Design Patterns Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts Create a Versioned System for the Case Example Session 5: Organising & Preparation Manage Data in ML Ops: Organize, Prepare & Reproduce Training Data Prep: Extraction, Sampling, Splitting & Windowing Component Setup: Labelling, Preprocessing, & Augmentation Modularise & isolate components for data preparation Session 6: Experiments & Training Baselines: Heuristic-driven, Simple ML baselines Setup for Rapid Experiments: Repeatable and Reproducible Experiment Tracking: Model Parameters, Optimisation Organise & Package for Data, Compute, and Infrastructure needs Session 7: Evaluation & Metrics Coarse-grained vs Fine-grained Model Evaluation Evaluation for Robustness: Stratification & Slicing, Confidence Metrics vs. Business Objectives & Interpretations Tracking evaluation metrics for coverage and failure cases Session 8: Testing & Validation Code Testing: Unit, Integration, System & Acceptance Data Testing: Schema, Expectations, Quality, Distribution Skew Model Testing: Training, Evaluation, Inference & Deployment Behavioural Testing: Invariance, Functionality, Directionality Session 9: Serving & Inference Align Objectives and Interfaces for Serving e.g Latency Inference Optimisation: Accelerators, Model Quantisation / Distillation Design for Production: Packages, Executables and Artefacts Thinking in Stages: Test, Canary, Development, Production Session 10: Monitoring & Feedback Performance monitoring and Post-Hoc Evaluation Predictive monitoring: Delayed Outcomes, Importance Weighting Measuring & Locating Drift: Data (Label & Feature), Concept Feedback loop: Integrating with Application Logging and Monitoring Session 11: Levels & Practice Advanced Levels of Maturity for MLOps Dataset Management: Catalogs, Quality, Lineage Feature Stores and Pipeline Orchestration Continuous Monitoring and Automated Triggering Session 12: Best Practices Advanced Case Study of a Successful MLOps Review of Best Practices for MLOps in Production Moving towards Continual Learning Data-centric Systems Learning Path and Way Forward","title":"Practical ML Operations (MLOps)"},{"location":"#practical-ml-operations-mlops","text":"!!! note This is work in progress.","title":"Practical ML Operations (MLOps)"},{"location":"#scope-session-plan","text":"Day 1: MLOps Landscape and Building Reproducible Prototype Day 2: Continuous Training - Data, Model, Experiments & Testing Day 3: Continuous Deployment - Serving, Inference, Feedback & System Performance","title":"Scope &amp; Session Plan"},{"location":"#session-1-introduction-concepts","text":"Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure","title":"Session 1: Introduction &amp; Concepts"},{"location":"#session-2-systems-operations","text":"Current Landscape and Maturity Level for ML Ops Define ML Ops: Requirements, Considerations, Interfaces, Data & Model Understand Process, Skills, and Tooling to move across Maturity Levels View Case Studies and Best Practices of ML Ops from the Industry","title":"Session 2: Systems &amp; Operations"},{"location":"#session-3-prototype-modularize","text":"Build a basic task-focused ML model for the case problem Create, build and deploy a local serving prototype ML system Modularize the Prototype Code, Data and Model for improved iteration Understand Trade-offs and Benefits of Modularized Approach","title":"Session 3: Prototype &amp; Modularize"},{"location":"#session-4-versioning-components","text":"Versioning in ML Ops: Workflow & Components Declarative Code, Data & Model Design Patterns Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts Create a Versioned System for the Case Example","title":"Session 4: Versioning &amp; Components"},{"location":"#session-5-organising-preparation","text":"Manage Data in ML Ops: Organize, Prepare & Reproduce Training Data Prep: Extraction, Sampling, Splitting & Windowing Component Setup: Labelling, Preprocessing, & Augmentation Modularise & isolate components for data preparation","title":"Session 5: Organising &amp; Preparation"},{"location":"#session-6-experiments-training","text":"Baselines: Heuristic-driven, Simple ML baselines Setup for Rapid Experiments: Repeatable and Reproducible Experiment Tracking: Model Parameters, Optimisation Organise & Package for Data, Compute, and Infrastructure needs","title":"Session 6: Experiments &amp; Training"},{"location":"#session-7-evaluation-metrics","text":"Coarse-grained vs Fine-grained Model Evaluation Evaluation for Robustness: Stratification & Slicing, Confidence Metrics vs. Business Objectives & Interpretations Tracking evaluation metrics for coverage and failure cases","title":"Session 7: Evaluation &amp; Metrics"},{"location":"#session-8-testing-validation","text":"Code Testing: Unit, Integration, System & Acceptance Data Testing: Schema, Expectations, Quality, Distribution Skew Model Testing: Training, Evaluation, Inference & Deployment Behavioural Testing: Invariance, Functionality, Directionality","title":"Session 8: Testing &amp; Validation"},{"location":"#session-9-serving-inference","text":"Align Objectives and Interfaces for Serving e.g Latency Inference Optimisation: Accelerators, Model Quantisation / Distillation Design for Production: Packages, Executables and Artefacts Thinking in Stages: Test, Canary, Development, Production","title":"Session 9: Serving &amp; Inference"},{"location":"#session-10-monitoring-feedback","text":"Performance monitoring and Post-Hoc Evaluation Predictive monitoring: Delayed Outcomes, Importance Weighting Measuring & Locating Drift: Data (Label & Feature), Concept Feedback loop: Integrating with Application Logging and Monitoring","title":"Session 10: Monitoring &amp; Feedback"},{"location":"#session-11-levels-practice","text":"Advanced Levels of Maturity for MLOps Dataset Management: Catalogs, Quality, Lineage Feature Stores and Pipeline Orchestration Continuous Monitoring and Automated Triggering","title":"Session 11: Levels &amp; Practice"},{"location":"#session-12-best-practices","text":"Advanced Case Study of a Successful MLOps Review of Best Practices for MLOps in Production Moving towards Continual Learning Data-centric Systems Learning Path and Way Forward","title":"Session 12: Best Practices"},{"location":"introduction-concepts/","text":"Introduction & Concepts Expectations No fancy ML How to build and deploy 1000s of models per day Expectations from the participants Latest ML Ops Process (Level 1, 2, 3 & 4) Algorithm - vertical vs. horizontal scaling ML ops system design vs. devops system design 200 DS -> 100 experiments (dev, prod, canary) data labelling on prem / cloud - latency issues production -> ML engineering requirements -> large scale / QPS create your own platform deep learning (scale to DL) data management -> schema / sheets on call -> monitoring and failures -> data purge issues deployment modes -> shadow mode? concept / data drift deploy -> CLI / NFR Testing -> dev / prod -> transparency of the deployed infrastructure testing -> unit testing (for data) validation Dev Ops vs. ML Ops ... ML Systems Level Level 1 Snowflake Business Requirements -> POC / Feasibility -> Data (ingest / refine or transform / preprocess) -> Model (Architecture / weights / hyper parameters / input-output) -> Deploy Level 2 Repeatable /Reproduceable same flow, different person/env Level 3 Repeatable over time Experiment: data=constant model=different serve->None Retraining: data=change model=same(weights change) serve -> new endpoint Multiple (t1, t2): data=constant model=constant serve->canary/dev/prod Level 4 Automatic Components Code Data Model Reproduce dependencies os version data snapshot data pipeline? random seed model hyperparams weights/checkpoints metrics? Versioning git commit hash Testing unit model (input,output) Automation Deploying Monitoring Docs Structure Approaches in Building ML Platforms The more traditional approach is to have the data scientists build a POC and hand it over to Data Engineers and ML Engineers to productionize their work. ---------------------- DS - Prototype ---------------------- DE | ME ---------------------- Platform ---------------------- Alternatively, the platform team can enable the data science team to build end to end applications, while taking take of compute and data abstractions. The data scientists can pick the right tool that works for them and platform team tries hard to enable that. ------------------------- User Model DS Version ------------------------- Platform Orchestrator Compute Data ------------------------- Not convinced, check out the post Aggressively Helpful Platform Teams from Stitchfix.","title":"Introduction & Concepts"},{"location":"introduction-concepts/#introduction-concepts","text":"","title":"Introduction &amp; Concepts"},{"location":"introduction-concepts/#expectations","text":"No fancy ML How to build and deploy 1000s of models per day","title":"Expectations"},{"location":"introduction-concepts/#expectations-from-the-participants","text":"Latest ML Ops Process (Level 1, 2, 3 & 4) Algorithm - vertical vs. horizontal scaling ML ops system design vs. devops system design 200 DS -> 100 experiments (dev, prod, canary) data labelling on prem / cloud - latency issues production -> ML engineering requirements -> large scale / QPS create your own platform deep learning (scale to DL) data management -> schema / sheets on call -> monitoring and failures -> data purge issues deployment modes -> shadow mode? concept / data drift deploy -> CLI / NFR Testing -> dev / prod -> transparency of the deployed infrastructure testing -> unit testing (for data) validation","title":"Expectations from the participants"},{"location":"introduction-concepts/#dev-ops-vs-ml-ops","text":"...","title":"Dev Ops vs. ML Ops"},{"location":"introduction-concepts/#ml-systems","text":"Level Level 1 Snowflake Business Requirements -> POC / Feasibility -> Data (ingest / refine or transform / preprocess) -> Model (Architecture / weights / hyper parameters / input-output) -> Deploy Level 2 Repeatable /Reproduceable same flow, different person/env Level 3 Repeatable over time Experiment: data=constant model=different serve->None Retraining: data=change model=same(weights change) serve -> new endpoint Multiple (t1, t2): data=constant model=constant serve->canary/dev/prod Level 4 Automatic Components Code Data Model Reproduce dependencies os version data snapshot data pipeline? random seed model hyperparams weights/checkpoints metrics? Versioning git commit hash Testing unit model (input,output) Automation Deploying Monitoring Docs Structure","title":"ML Systems"},{"location":"introduction-concepts/#approaches-in-building-ml-platforms","text":"The more traditional approach is to have the data scientists build a POC and hand it over to Data Engineers and ML Engineers to productionize their work. ---------------------- DS - Prototype ---------------------- DE | ME ---------------------- Platform ---------------------- Alternatively, the platform team can enable the data science team to build end to end applications, while taking take of compute and data abstractions. The data scientists can pick the right tool that works for them and platform team tries hard to enable that. ------------------------- User Model DS Version ------------------------- Platform Orchestrator Compute Data ------------------------- Not convinced, check out the post Aggressively Helpful Platform Teams from Stitchfix.","title":"Approaches in Building ML Platforms"},{"location":"pages/","text":"Practical ML Operations (MLOps) {% callout type=\"note\" %} This is work in progress. {% /callout %} Scope & Session Plan Day 1: MLOps Landscape and Building Reproducible Prototype Day 2: Continuous Training - Data, Model, Experiments & Testing Day 3: Continuous Deployment - Serving, Inference, Feedback & System Performance Session 1: Introduction & Concepts Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure Session 2: Systems & Operations Current Landscape and Maturity Level for ML Ops Define ML Ops: Requirements, Considerations, Interfaces, Data & Model Understand Process, Skills, and Tooling to move across Maturity Levels View Case Studies and Best Practices of ML Ops from the Industry Session 3: Prototype & Modularize Build a basic task-focused ML model for the case problem Create, build and deploy a local serving prototype ML system Modularize the Prototype Code, Data and Model for improved iteration Understand Trade-offs and Benefits of Modularized Approach Session 4: Versioning & Components Versioning in ML Ops: Workflow & Components Declarative Code, Data & Model Design Patterns Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts Create a Versioned System for the Case Example Session 5: Organising & Preparation Manage Data in ML Ops: Organize, Prepare & Reproduce Training Data Prep: Extraction, Sampling, Splitting & Windowing Component Setup: Labelling, Preprocessing, & Augmentation Modularise & isolate components for data preparation Session 6: Experiments & Training Baselines: Heuristic-driven, Simple ML baselines Setup for Rapid Experiments: Repeatable and Reproducible Experiment Tracking: Model Parameters, Optimisation Organise & Package for Data, Compute, and Infrastructure needs Session 7: Evaluation & Metrics Coarse-grained vs Fine-grained Model Evaluation Evaluation for Robustness: Stratification & Slicing, Confidence Metrics vs. Business Objectives & Interpretations Tracking evaluation metrics for coverage and failure cases Session 8: Testing & Validation Code Testing: Unit, Integration, System & Acceptance Data Testing: Schema, Expectations, Quality, Distribution Skew Model Testing: Training, Evaluation, Inference & Deployment Behavioural Testing: Invariance, Functionality, Directionality Session 9: Serving & Inference Align Objectives and Interfaces for Serving e.g Latency Inference Optimisation: Accelerators, Model Quantisation / Distillation Design for Production: Packages, Executables and Artefacts Thinking in Stages: Test, Canary, Development, Production Session 10: Monitoring & Feedback Performance monitoring and Post-Hoc Evaluation Predictive monitoring: Delayed Outcomes, Importance Weighting Measuring & Locating Drift: Data (Label & Feature), Concept Feedback loop: Integrating with Application Logging and Monitoring Session 11: Levels & Practice Advanced Levels of Maturity for MLOps Dataset Management: Catalogs, Quality, Lineage Feature Stores and Pipeline Orchestration Continuous Monitoring and Automated Triggering Session 12: Best Practices Advanced Case Study of a Successful MLOps Review of Best Practices for MLOps in Production Moving towards Continual Learning Data-centric Systems Learning Path and Way Forward","title":"**Practical ML Operations (MLOps)**"},{"location":"pages/#practical-ml-operations-mlops","text":"{% callout type=\"note\" %} This is work in progress. {% /callout %}","title":"Practical ML Operations (MLOps)"},{"location":"pages/#scope-session-plan","text":"Day 1: MLOps Landscape and Building Reproducible Prototype Day 2: Continuous Training - Data, Model, Experiments & Testing Day 3: Continuous Deployment - Serving, Inference, Feedback & System Performance","title":"Scope &amp; Session Plan"},{"location":"pages/#session-1-introduction-concepts","text":"Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure","title":"Session 1: Introduction &amp; Concepts"},{"location":"pages/#session-2-systems-operations","text":"Current Landscape and Maturity Level for ML Ops Define ML Ops: Requirements, Considerations, Interfaces, Data & Model Understand Process, Skills, and Tooling to move across Maturity Levels View Case Studies and Best Practices of ML Ops from the Industry","title":"Session 2: Systems &amp; Operations"},{"location":"pages/#session-3-prototype-modularize","text":"Build a basic task-focused ML model for the case problem Create, build and deploy a local serving prototype ML system Modularize the Prototype Code, Data and Model for improved iteration Understand Trade-offs and Benefits of Modularized Approach","title":"Session 3: Prototype &amp; Modularize"},{"location":"pages/#session-4-versioning-components","text":"Versioning in ML Ops: Workflow & Components Declarative Code, Data & Model Design Patterns Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts Create a Versioned System for the Case Example","title":"Session 4: Versioning &amp; Components"},{"location":"pages/#session-5-organising-preparation","text":"Manage Data in ML Ops: Organize, Prepare & Reproduce Training Data Prep: Extraction, Sampling, Splitting & Windowing Component Setup: Labelling, Preprocessing, & Augmentation Modularise & isolate components for data preparation","title":"Session 5: Organising &amp; Preparation"},{"location":"pages/#session-6-experiments-training","text":"Baselines: Heuristic-driven, Simple ML baselines Setup for Rapid Experiments: Repeatable and Reproducible Experiment Tracking: Model Parameters, Optimisation Organise & Package for Data, Compute, and Infrastructure needs","title":"Session 6: Experiments &amp; Training"},{"location":"pages/#session-7-evaluation-metrics","text":"Coarse-grained vs Fine-grained Model Evaluation Evaluation for Robustness: Stratification & Slicing, Confidence Metrics vs. Business Objectives & Interpretations Tracking evaluation metrics for coverage and failure cases","title":"Session 7: Evaluation &amp; Metrics"},{"location":"pages/#session-8-testing-validation","text":"Code Testing: Unit, Integration, System & Acceptance Data Testing: Schema, Expectations, Quality, Distribution Skew Model Testing: Training, Evaluation, Inference & Deployment Behavioural Testing: Invariance, Functionality, Directionality","title":"Session 8: Testing &amp; Validation"},{"location":"pages/#session-9-serving-inference","text":"Align Objectives and Interfaces for Serving e.g Latency Inference Optimisation: Accelerators, Model Quantisation / Distillation Design for Production: Packages, Executables and Artefacts Thinking in Stages: Test, Canary, Development, Production","title":"Session 9: Serving &amp; Inference"},{"location":"pages/#session-10-monitoring-feedback","text":"Performance monitoring and Post-Hoc Evaluation Predictive monitoring: Delayed Outcomes, Importance Weighting Measuring & Locating Drift: Data (Label & Feature), Concept Feedback loop: Integrating with Application Logging and Monitoring","title":"Session 10: Monitoring &amp; Feedback"},{"location":"pages/#session-11-levels-practice","text":"Advanced Levels of Maturity for MLOps Dataset Management: Catalogs, Quality, Lineage Feature Stores and Pipeline Orchestration Continuous Monitoring and Automated Triggering","title":"Session 11: Levels &amp; Practice"},{"location":"pages/#session-12-best-practices","text":"Advanced Case Study of a Successful MLOps Review of Best Practices for MLOps in Production Moving towards Continual Learning Data-centric Systems Learning Path and Way Forward","title":"Session 12: Best Practices"},{"location":"pages/introduction-concepts/","text":"Introduction & Concepts Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure Intro Instructor Introduction Participant Introduction Experience ML Experience Personal ML Workflow Org ML Workflow / Infra / Setup for ML Right now Sample Project in Production (2 - 3 Examples) Type: ML / DL / Forecasting Process Data Storage / ELT Compute Abstraction (Laptop / Cloud) Productionion WorkFlow Monitoring & ReTraining Grouping People with Diverse Background Course Expecation Expectation Not focussed on ML/DL model building Framework, System Agnostics What is the Focus? Difference between PoC and Production ML System Building the Model (Algorithm) is easy? Solving real world problems Success of ML Projects in Production is low? Software Engineering vs. Programming -> Change over Time Objectives Compare: Project vs. Product/System Approach Comparing: Exploration vs. Production in ML Objective | Model Performance | Business Performance Computation Priority | Fast Training | Fast Inference, low latency Data | Clean / Static / Historical | Messy / Dynamic / + Streaming / Changing / Privacy Fairness / Interepretability | Maybe | Important Comparing: Software Systems vs. ML Systems Diagram for Software Systems Code - Modular Design Data - Separate Tests (Unit Test, Integration Test) Build (Package) Deployment (Package) Deploying Reliably is hard (Case Example) Deploy Multiple times at a times (DevOps Evolution) Speed of delivery Evolution of Building Software Evolution over years: Shift from separate Dev QA, SysAdmin -> DevOps Role -> CI/CD 1 deploy a month to 100s of deploy per day UI Developments Model Update - DevOps vs Normal What is different Team Skills: Roles Development Testing Deployment Production Requirements of an ML Systems Reliable: Correct function at desired level of performance and failure resistance Fail Silently Scalability: Scale Up and Down: Data Volume, Traffic Volume and Complexity Maintainbility: People / Skills - Hetrogenous Tools & Background Adaptibility: Data Distribution and Business Requirement - React Quickly Exercise Real Life Example (Take Three - Select One) Structured Data Simple Problem (not multi-objective) Inference at Real-Time with Considerations: Framing, Objectives, Constraints & Phases Framing: Right Problem for the ML Objection Example: MultiClass Decision Boundary (Which App to open) Objective: ML Objective: Performance, Latency Business Objective: Cost, ROI, Regulation Baseline, Threshold, False Negative, Interpretability, Confidence Constrain: Time & Budget (More Powerful Machine, More Experiment), Privacy Phased: Heuristic, Simple, ML, Complex Thinking in Systems: Application, Business and Infrastructure Link to the Exercise","title":"Introduction & Concepts"},{"location":"pages/introduction-concepts/#introduction-concepts","text":"Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure","title":"Introduction &amp; Concepts"},{"location":"pages/introduction-concepts/#intro","text":"Instructor Introduction Participant Introduction Experience ML Experience Personal ML Workflow Org ML Workflow / Infra / Setup for ML Right now Sample Project in Production (2 - 3 Examples) Type: ML / DL / Forecasting Process Data Storage / ELT Compute Abstraction (Laptop / Cloud) Productionion WorkFlow Monitoring & ReTraining Grouping People with Diverse Background","title":"Intro"},{"location":"pages/introduction-concepts/#course-expecation","text":"Expectation Not focussed on ML/DL model building Framework, System Agnostics","title":"Course Expecation"},{"location":"pages/introduction-concepts/#what-is-the-focus","text":"Difference between PoC and Production ML System Building the Model (Algorithm) is easy? Solving real world problems Success of ML Projects in Production is low? Software Engineering vs. Programming -> Change over Time","title":"What is the Focus?"},{"location":"pages/introduction-concepts/#objectives","text":"","title":"Objectives"},{"location":"pages/introduction-concepts/#compare-project-vs-productsystem-approach","text":"","title":"Compare: Project vs. Product/System Approach"},{"location":"pages/introduction-concepts/#comparing-exploration-vs-production-in-ml","text":"Objective | Model Performance | Business Performance Computation Priority | Fast Training | Fast Inference, low latency Data | Clean / Static / Historical | Messy / Dynamic / + Streaming / Changing / Privacy Fairness / Interepretability | Maybe | Important","title":"Comparing: Exploration vs. Production in ML"},{"location":"pages/introduction-concepts/#comparing-software-systems-vs-ml-systems","text":"Diagram for Software Systems Code - Modular Design Data - Separate Tests (Unit Test, Integration Test) Build (Package) Deployment (Package) Deploying Reliably is hard (Case Example) Deploy Multiple times at a times (DevOps Evolution) Speed of delivery","title":"Comparing: Software Systems vs. ML Systems"},{"location":"pages/introduction-concepts/#evolution-of-building-software","text":"Evolution over years: Shift from separate Dev QA, SysAdmin -> DevOps Role -> CI/CD 1 deploy a month to 100s of deploy per day UI Developments Model Update - DevOps vs Normal What is different Team Skills: Roles Development Testing Deployment Production","title":"Evolution of Building Software"},{"location":"pages/introduction-concepts/#requirements-of-an-ml-systems","text":"Reliable: Correct function at desired level of performance and failure resistance Fail Silently Scalability: Scale Up and Down: Data Volume, Traffic Volume and Complexity Maintainbility: People / Skills - Hetrogenous Tools & Background Adaptibility: Data Distribution and Business Requirement - React Quickly","title":"Requirements of an ML Systems"},{"location":"pages/introduction-concepts/#exercise","text":"Real Life Example (Take Three - Select One) Structured Data Simple Problem (not multi-objective) Inference at Real-Time with","title":"Exercise"},{"location":"pages/introduction-concepts/#considerations-framing-objectives-constraints-phases","text":"Framing: Right Problem for the ML Objection Example: MultiClass Decision Boundary (Which App to open) Objective: ML Objective: Performance, Latency Business Objective: Cost, ROI, Regulation Baseline, Threshold, False Negative, Interpretability, Confidence Constrain: Time & Budget (More Powerful Machine, More Experiment), Privacy Phased: Heuristic, Simple, ML, Complex","title":"Considerations: Framing, Objectives, Constraints &amp; Phases"},{"location":"pages/introduction-concepts/#thinking-in-systems-application-business-and-infrastructure","text":"Link to the Exercise","title":"Thinking in Systems: Application, Business and Infrastructure"},{"location":"pages/learning-obectives/","text":"Learning Objectives Remember -> Recall / Retrieve / Reproduce Understand -> Explain / Demonstrate / Discuss Apply -> Use / Solve Analyze -> Examine / Break Evaluate -> Defend / Justify Create -> Generate / Compile","title":"Learning Objectives"},{"location":"pages/learning-obectives/#learning-objectives","text":"Remember -> Recall / Retrieve / Reproduce Understand -> Explain / Demonstrate / Discuss Apply -> Use / Solve Analyze -> Examine / Break Evaluate -> Defend / Justify Create -> Generate / Compile","title":"Learning Objectives"},{"location":"pages/levels/","text":"Levels Maturity Map Level 1, 2 ,3, 4, 5 Feature Engineering Pipeline Data Ingestion Data Loading Data Preloading Data Experimentation Data Analysis Data Preparation & Validation Model Training Model Validation Model Packaging Automated Ml Pipeline","title":"Levels Maturity Map"},{"location":"pages/levels/#levels-maturity-map","text":"Level 1, 2 ,3, 4, 5 Feature Engineering Pipeline Data Ingestion Data Loading Data Preloading Data Experimentation Data Analysis Data Preparation & Validation Model Training Model Validation Model Packaging Automated Ml Pipeline","title":"Levels Maturity Map"},{"location":"pages/systems-operations/","text":"Systems & Operations Levels of Maturity Benifits & Challenges at each level Level 1: Manual, Notebook driven (Visual) Notebook for Data Preparation, Model Traning, Model Validation Script for Serving Example: Classification Penguin Level 2: Reproducible ML Project Structure (Convention / Practices) All Code in Version All Data Source & Model are Tagged Versioning Code Level 3: Modular ML Data Validation, Model Validation Modular Steps Unit Testing Experiment Tracking Model Registry Data Catalog Level 4: Automated ML Code and Pipeline are Version Control Compute Platform (Orchestration) Modular Pipeline Experiment Tracking Data Lineage Feature Store (PreCompute and Metadata) Model Repository Serving Platform - Auto deploy Monitoriing & Alerting","title":"Systems & Operations"},{"location":"pages/systems-operations/#systems-operations","text":"","title":"Systems &amp; Operations"},{"location":"pages/systems-operations/#levels-of-maturity","text":"Benifits & Challenges at each level Level 1: Manual, Notebook driven (Visual) Notebook for Data Preparation, Model Traning, Model Validation Script for Serving Example: Classification Penguin Level 2: Reproducible ML Project Structure (Convention / Practices) All Code in Version All Data Source & Model are Tagged Versioning Code Level 3: Modular ML Data Validation, Model Validation Modular Steps Unit Testing Experiment Tracking Model Registry Data Catalog Level 4: Automated ML Code and Pipeline are Version Control Compute Platform (Orchestration) Modular Pipeline Experiment Tracking Data Lineage Feature Store (PreCompute and Metadata) Model Repository Serving Platform - Auto deploy Monitoriing & Alerting","title":"Levels of Maturity"}]}