{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Practical ML Operations (MLOps) Note This is work in progress. Scope & Session Plan Day 1: MLOps Landscape and Building Reproducible Prototype Day 2: Continuous Training - Data, Model, Experiments & Testing Day 3: Continuous Deployment - Serving, Inference, Feedback & System Performance Session 1: Introduction & Concepts Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure Session 2: Systems & Operations Current Landscape and Maturity Level for ML Ops Define ML Ops: Requirements, Considerations, Interfaces, Data & Model Understand Process, Skills, and Tooling to move across Maturity Levels View Case Studies and Best Practices of ML Ops from the Industry Session 3: Prototype & Modularize Build a basic task-focused ML model for the case problem Create, build and deploy a local serving prototype ML system Modularize the Prototype Code, Data and Model for improved iteration Understand Trade-offs and Benefits of Modularized Approach Session 4: Versioning & Components Versioning in ML Ops: Workflow & Components Declarative Code, Data & Model Design Patterns Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts Create a Versioned System for the Case Example Session 5: Organising & Preparation Manage Data in ML Ops: Organize, Prepare & Reproduce Training Data Prep: Extraction, Sampling, Splitting & Windowing Component Setup: Labelling, Preprocessing, & Augmentation Modularise & isolate components for data preparation Session 6: Experiments & Training Baselines: Heuristic-driven, Simple ML baselines Setup for Rapid Experiments: Repeatable and Reproducible Experiment Tracking: Model Parameters, Optimisation Organise & Package for Data, Compute, and Infrastructure needs Session 7: Evaluation & Metrics Coarse-grained vs Fine-grained Model Evaluation Evaluation for Robustness: Stratification & Slicing, Confidence Metrics vs. Business Objectives & Interpretations Tracking evaluation metrics for coverage and failure cases Session 8: Testing & Validation Code Testing: Unit, Integration, System & Acceptance Data Testing: Schema, Expectations, Quality, Distribution Skew Model Testing: Training, Evaluation, Inference & Deployment Behavioural Testing: Invariance, Functionality, Directionality Session 9: Serving & Inference Align Objectives and Interfaces for Serving e.g Latency Inference Optimisation: Accelerators, Model Quantisation / Distillation Design for Production: Packages, Executables and Artefacts Thinking in Stages: Test, Canary, Development, Production Session 10: Monitoring & Feedback Performance monitoring and Post-Hoc Evaluation Predictive monitoring: Delayed Outcomes, Importance Weighting Measuring & Locating Drift: Data (Label & Feature), Concept Feedback loop: Integrating with Application Logging and Monitoring Session 11: Levels & Practice Advanced Levels of Maturity for MLOps Dataset Management: Catalogs, Quality, Lineage Feature Stores and Pipeline Orchestration Continuous Monitoring and Automated Triggering Session 12: Best Practices Advanced Case Study of a Successful MLOps Review of Best Practices for MLOps in Production Moving towards Continual Learning Data-centric Systems Learning Path and Way Forward","title":"Practical ML Operations (MLOps)"},{"location":"#practical-ml-operations-mlops","text":"Note This is work in progress.","title":"Practical ML Operations (MLOps)"},{"location":"#scope-session-plan","text":"Day 1: MLOps Landscape and Building Reproducible Prototype Day 2: Continuous Training - Data, Model, Experiments & Testing Day 3: Continuous Deployment - Serving, Inference, Feedback & System Performance","title":"Scope &amp; Session Plan"},{"location":"#session-1-introduction-concepts","text":"Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure","title":"Session 1: Introduction &amp; Concepts"},{"location":"#session-2-systems-operations","text":"Current Landscape and Maturity Level for ML Ops Define ML Ops: Requirements, Considerations, Interfaces, Data & Model Understand Process, Skills, and Tooling to move across Maturity Levels View Case Studies and Best Practices of ML Ops from the Industry","title":"Session 2: Systems &amp; Operations"},{"location":"#session-3-prototype-modularize","text":"Build a basic task-focused ML model for the case problem Create, build and deploy a local serving prototype ML system Modularize the Prototype Code, Data and Model for improved iteration Understand Trade-offs and Benefits of Modularized Approach","title":"Session 3: Prototype &amp; Modularize"},{"location":"#session-4-versioning-components","text":"Versioning in ML Ops: Workflow & Components Declarative Code, Data & Model Design Patterns Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts Create a Versioned System for the Case Example","title":"Session 4: Versioning &amp; Components"},{"location":"#session-5-organising-preparation","text":"Manage Data in ML Ops: Organize, Prepare & Reproduce Training Data Prep: Extraction, Sampling, Splitting & Windowing Component Setup: Labelling, Preprocessing, & Augmentation Modularise & isolate components for data preparation","title":"Session 5: Organising &amp; Preparation"},{"location":"#session-6-experiments-training","text":"Baselines: Heuristic-driven, Simple ML baselines Setup for Rapid Experiments: Repeatable and Reproducible Experiment Tracking: Model Parameters, Optimisation Organise & Package for Data, Compute, and Infrastructure needs","title":"Session 6: Experiments &amp; Training"},{"location":"#session-7-evaluation-metrics","text":"Coarse-grained vs Fine-grained Model Evaluation Evaluation for Robustness: Stratification & Slicing, Confidence Metrics vs. Business Objectives & Interpretations Tracking evaluation metrics for coverage and failure cases","title":"Session 7: Evaluation &amp; Metrics"},{"location":"#session-8-testing-validation","text":"Code Testing: Unit, Integration, System & Acceptance Data Testing: Schema, Expectations, Quality, Distribution Skew Model Testing: Training, Evaluation, Inference & Deployment Behavioural Testing: Invariance, Functionality, Directionality","title":"Session 8: Testing &amp; Validation"},{"location":"#session-9-serving-inference","text":"Align Objectives and Interfaces for Serving e.g Latency Inference Optimisation: Accelerators, Model Quantisation / Distillation Design for Production: Packages, Executables and Artefacts Thinking in Stages: Test, Canary, Development, Production","title":"Session 9: Serving &amp; Inference"},{"location":"#session-10-monitoring-feedback","text":"Performance monitoring and Post-Hoc Evaluation Predictive monitoring: Delayed Outcomes, Importance Weighting Measuring & Locating Drift: Data (Label & Feature), Concept Feedback loop: Integrating with Application Logging and Monitoring","title":"Session 10: Monitoring &amp; Feedback"},{"location":"#session-11-levels-practice","text":"Advanced Levels of Maturity for MLOps Dataset Management: Catalogs, Quality, Lineage Feature Stores and Pipeline Orchestration Continuous Monitoring and Automated Triggering","title":"Session 11: Levels &amp; Practice"},{"location":"#session-12-best-practices","text":"Advanced Case Study of a Successful MLOps Review of Best Practices for MLOps in Production Moving towards Continual Learning Data-centric Systems Learning Path and Way Forward","title":"Session 12: Best Practices"},{"location":"day1/","text":"Day 1 Expectations No fancy ML How to build and deploy 1000s of models per day Expectations from the participants Latest ML Ops Process (Level 1, 2, 3 & 4) Algorithm - vertical vs. horizontal scaling ML ops system design vs. devops system design 200 DS -> 100 experiments (dev, prod, canary) data labelling on prem / cloud - latency issues production -> ML engineering requirements -> large scale / QPS create your own platform deep learning (scale to DL) data management -> schema / sheets on call -> monitoring and failures -> data purge issues deployment modes -> shadow mode? concept / data drift deploy -> CLI / NFR Testing -> dev / prod -> transparency of the deployed infrastructure testing -> unit testing (for data) validation Dev Ops vs. ML Ops ... ML Systems Level Level 1 Snowflake Business Requirements -> POC / Feasibility -> Data (ingest / refine or transform / preprocess) -> Model (Architecture / weights / hyper parameters / input-output) -> Deploy Level 2 Repeatable /Reproduceable same flow, different person/env Level 3 Repeatable over time Experiment: data=constant model=different serve->None Retraining: data=change model=same(weights change) serve -> new endpoint Multiple (t1, t2): data=constant model=constant serve->canary/dev/prod Level 4 Automatic Components Code Data Model Reproduce dependencies os version data snapshot data pipeline? random seed model hyperparams weights/checkpoints metrics? Versioning git commit hash Testing unit model (input,output) Automation Deploying Monitoring Docs Structure Approaches in Building ML Platforms The more traditional approach is to have the data scientists build a POC and hand it over to Data Engineers and ML Engineers to productionize their work. ---------------------- DS - Prototype ---------------------- DE | ME ---------------------- Platform ---------------------- Alternatively, the platform team can enable the data science team to build end to end applications, while taking take of compute and data abstractions. The data scientists can pick the right tool that works for them and platform team tries hard to enable that. ------------------------- User Model DS Version ------------------------- Platform Orchestrator Compute Data ------------------------- Not convinced? Check out the post Aggressively Helpful Platform Teams from Stitchfix.","title":"Day 1"},{"location":"day1/#day-1","text":"","title":"Day 1"},{"location":"day1/#expectations","text":"No fancy ML How to build and deploy 1000s of models per day","title":"Expectations"},{"location":"day1/#expectations-from-the-participants","text":"Latest ML Ops Process (Level 1, 2, 3 & 4) Algorithm - vertical vs. horizontal scaling ML ops system design vs. devops system design 200 DS -> 100 experiments (dev, prod, canary) data labelling on prem / cloud - latency issues production -> ML engineering requirements -> large scale / QPS create your own platform deep learning (scale to DL) data management -> schema / sheets on call -> monitoring and failures -> data purge issues deployment modes -> shadow mode? concept / data drift deploy -> CLI / NFR Testing -> dev / prod -> transparency of the deployed infrastructure testing -> unit testing (for data) validation","title":"Expectations from the participants"},{"location":"day1/#dev-ops-vs-ml-ops","text":"...","title":"Dev Ops vs. ML Ops"},{"location":"day1/#ml-systems","text":"Level Level 1 Snowflake Business Requirements -> POC / Feasibility -> Data (ingest / refine or transform / preprocess) -> Model (Architecture / weights / hyper parameters / input-output) -> Deploy Level 2 Repeatable /Reproduceable same flow, different person/env Level 3 Repeatable over time Experiment: data=constant model=different serve->None Retraining: data=change model=same(weights change) serve -> new endpoint Multiple (t1, t2): data=constant model=constant serve->canary/dev/prod Level 4 Automatic Components Code Data Model Reproduce dependencies os version data snapshot data pipeline? random seed model hyperparams weights/checkpoints metrics? Versioning git commit hash Testing unit model (input,output) Automation Deploying Monitoring Docs Structure","title":"ML Systems"},{"location":"day1/#approaches-in-building-ml-platforms","text":"The more traditional approach is to have the data scientists build a POC and hand it over to Data Engineers and ML Engineers to productionize their work. ---------------------- DS - Prototype ---------------------- DE | ME ---------------------- Platform ---------------------- Alternatively, the platform team can enable the data science team to build end to end applications, while taking take of compute and data abstractions. The data scientists can pick the right tool that works for them and platform team tries hard to enable that. ------------------------- User Model DS Version ------------------------- Platform Orchestrator Compute Data ------------------------- Not convinced? Check out the post Aggressively Helpful Platform Teams from Stitchfix.","title":"Approaches in Building ML Platforms"},{"location":"day2/","text":"Day 2 Model Envelope Install model-envelope library using the following command: $ pip install -U git+https://github.com/anandology/model-envelope Evaluation & Metrics ML Models in production: Returns Returns: Class - option / Comment - text -> Allow/block next step in UI Invoice PDF: Item detail json (#, date, name) Delivery time prediction: Location, seller/source -> #days Customer Address -> Lat/long (gold dataset: address -> lat/long, NLP classification) Forecasting -> Demand next one month Cancellation before delivery: user pincode -> COD yes/no Question -> retrive similar answered questions Testing & Validation Fail Early, Fail Often Code Testing: Unit, Integration, System & Acceptance Test each of the training steps is working as expected Test if the entire training pipeline is working without any errors Test if the training and using the model inference is working as expected How do you do each of these? Would keeping a sample dataset help? Do you also need to keep expected results? Data Testing: Schema, Expectations, Quality, Distribution Skew How to make sure the schema of input data is as expected? Can you add some expectations on the data like the min/max values, allowed patterns or possible options for a categorical data? Would a check on the quality of data be useful? Like the % of missing/invalid values? Can you make sure the distibution of data has not drifted too much from your first trianing? The Python library Pandera has support for all these kinds of checks. Model Testing: Training, Evaluation, Inference & Deployment Can you make sure the training accuracy is more than a minimum expected value? Can you make sure thr training accuracy is always more than the previous run? Is there a test suite that you can run on a deployed model to ensure it is working correctly before taking it live? Behavioural Testing: Invariance, Functionality, Directionality How can you test the behaviour of your model? invariance: what are the inputs that should produce the same inference directionality: what are the inputs that should not produce the same inference functionality: minimum functionality that you expect the model to always predict as expected Exercise: Classification of Diamonds Look at the diamonds example in the mlops repository . In this example, all the code is in a notebook. make the training process repeatable and reproduceable by moving the code from notebook to a git repo. You can use the cookiecutter-mlops to get started. add tests to verify the code is working as expected add data valiadtion as part of the training pipeline to alert for schema changes, quality or change in the data distribution how can you test the model? Also, the dataset has data for three seperate months. Run training for each of this data chunk and make sure that the data validation is happening as expected.","title":"Day 2"},{"location":"day2/#day-2","text":"","title":"Day 2"},{"location":"day2/#model-envelope","text":"Install model-envelope library using the following command: $ pip install -U git+https://github.com/anandology/model-envelope","title":"Model Envelope"},{"location":"day2/#evaluation-metrics","text":"ML Models in production: Returns Returns: Class - option / Comment - text -> Allow/block next step in UI Invoice PDF: Item detail json (#, date, name) Delivery time prediction: Location, seller/source -> #days Customer Address -> Lat/long (gold dataset: address -> lat/long, NLP classification) Forecasting -> Demand next one month Cancellation before delivery: user pincode -> COD yes/no Question -> retrive similar answered questions","title":"Evaluation &amp; Metrics"},{"location":"day2/#testing-validation","text":"Fail Early, Fail Often","title":"Testing &amp; Validation"},{"location":"day2/#code-testing-unit-integration-system-acceptance","text":"Test each of the training steps is working as expected Test if the entire training pipeline is working without any errors Test if the training and using the model inference is working as expected How do you do each of these? Would keeping a sample dataset help? Do you also need to keep expected results?","title":"Code Testing: Unit, Integration, System &amp; Acceptance"},{"location":"day2/#data-testing-schema-expectations-quality-distribution-skew","text":"How to make sure the schema of input data is as expected? Can you add some expectations on the data like the min/max values, allowed patterns or possible options for a categorical data? Would a check on the quality of data be useful? Like the % of missing/invalid values? Can you make sure the distibution of data has not drifted too much from your first trianing? The Python library Pandera has support for all these kinds of checks.","title":"Data Testing: Schema, Expectations, Quality, Distribution Skew"},{"location":"day2/#model-testing-training-evaluation-inference-deployment","text":"Can you make sure the training accuracy is more than a minimum expected value? Can you make sure thr training accuracy is always more than the previous run? Is there a test suite that you can run on a deployed model to ensure it is working correctly before taking it live?","title":"Model Testing: Training, Evaluation, Inference &amp; Deployment"},{"location":"day2/#behavioural-testing-invariance-functionality-directionality","text":"How can you test the behaviour of your model? invariance: what are the inputs that should produce the same inference directionality: what are the inputs that should not produce the same inference functionality: minimum functionality that you expect the model to always predict as expected Exercise: Classification of Diamonds Look at the diamonds example in the mlops repository . In this example, all the code is in a notebook. make the training process repeatable and reproduceable by moving the code from notebook to a git repo. You can use the cookiecutter-mlops to get started. add tests to verify the code is working as expected add data valiadtion as part of the training pipeline to alert for schema changes, quality or change in the data distribution how can you test the model? Also, the dataset has data for three seperate months. Run training for each of this data chunk and make sure that the data validation is happening as expected.","title":"Behavioural Testing: Invariance, Functionality, Directionality"},{"location":"day3/","text":"Day 3 Experiment Tracking with ML Flow ML Flow is a popular open-source platform to manage ML lifecycle, including experiment tracking and a central model registry. Concepts Experiments ML Flow is organized by experiments. Each experiment is typically made of multiple runs of training job with possibly different parameters or data. Run Each run is one execution of a trainining job. A run can log the model parameters, metrics and artifacts. Artifacts Artifacts are the files that we would like to attach to a training run. These are typically the generated by the training, like a model file or an evaluation report. Model Registry Apart from storing model as part of each training run, mlflow also provides an option to add a model to a central model registry and make it possible to search the available models in the model registry. Getting Started Install mlflow python package to get started. $ pip install mlflow For this training, we've setup a ML flow server install for all of you to use at https://mlflow.pipal.in/ . You can open that web site and look at the existing experiments and runs. Let's try a sample run. You could try this in your notebook. import mlflow mlflow.set_tracking_uri(\"https://mlflow.pipal.in/\") # You can pick a different experiment name mlflow.set_experiment(\"hello-world-anand\") with mlflow.start_run(): # log a param mlflow.log_param(\"param1\", 10) # log a metric mlflow.log_metric(\"accuracy\", 99.2) # log an artifact with open(\"hello.txt\", \"w\") as f: f.write(\"hello, world!\") mlflow.log_artifact(\"hello.txt\") print(\"done\") Once you run this code, you'll see a new experiment and a run recorded for it. If you run the code again, you'll see one more run under the same experiment. What ever params, metrics and artifacts that we logged will be shown with the run. Training a scikit-learn model Mlflow supports autologging of many popular ML libraries. We just need to make a function call to enable autologging. import pandas as pd import mlflow from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics mlflow.set_tracking_uri(\"https://mlflow.pipal.in/\") mlflow.set_experiment(\"Penguins Classification\") mlflow.autolog() print(\"reading the data...\") url = \"https://raw.githubusercontent.com/amitkaps/mlops/main/datasets/palmerpenguins/penguins.csv\" df = pd.read_csv(url) print(\"preparing the data...\") df = df.dropna() columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g'] X = df[columns] y = df.species print(\"splitting the data into train and test...\") X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) with mlflow.start_run(): print(\"training the model...\") params = dict(random_state=44, max_depth=6) model = DecisionTreeClassifier(**params) model.fit(X_train, y_train) mlflow.sklearn.log_model( sk_model=model, artifact_path=\"model\", registered_model_name=\"penguins-classfier\" ) print(\"validating...\") y_pred = model.predict(X_test) test_accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred) print(\"Accuracy on test data:\", test_accuracy)","title":"Day 3"},{"location":"day3/#day-3","text":"","title":"Day 3"},{"location":"day3/#experiment-tracking-with-ml-flow","text":"ML Flow is a popular open-source platform to manage ML lifecycle, including experiment tracking and a central model registry.","title":"Experiment Tracking with ML Flow"},{"location":"day3/#concepts","text":"Experiments ML Flow is organized by experiments. Each experiment is typically made of multiple runs of training job with possibly different parameters or data. Run Each run is one execution of a trainining job. A run can log the model parameters, metrics and artifacts. Artifacts Artifacts are the files that we would like to attach to a training run. These are typically the generated by the training, like a model file or an evaluation report. Model Registry Apart from storing model as part of each training run, mlflow also provides an option to add a model to a central model registry and make it possible to search the available models in the model registry.","title":"Concepts"},{"location":"day3/#getting-started","text":"Install mlflow python package to get started. $ pip install mlflow For this training, we've setup a ML flow server install for all of you to use at https://mlflow.pipal.in/ . You can open that web site and look at the existing experiments and runs. Let's try a sample run. You could try this in your notebook. import mlflow mlflow.set_tracking_uri(\"https://mlflow.pipal.in/\") # You can pick a different experiment name mlflow.set_experiment(\"hello-world-anand\") with mlflow.start_run(): # log a param mlflow.log_param(\"param1\", 10) # log a metric mlflow.log_metric(\"accuracy\", 99.2) # log an artifact with open(\"hello.txt\", \"w\") as f: f.write(\"hello, world!\") mlflow.log_artifact(\"hello.txt\") print(\"done\") Once you run this code, you'll see a new experiment and a run recorded for it. If you run the code again, you'll see one more run under the same experiment. What ever params, metrics and artifacts that we logged will be shown with the run.","title":"Getting Started"},{"location":"day3/#training-a-scikit-learn-model","text":"Mlflow supports autologging of many popular ML libraries. We just need to make a function call to enable autologging. import pandas as pd import mlflow from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics mlflow.set_tracking_uri(\"https://mlflow.pipal.in/\") mlflow.set_experiment(\"Penguins Classification\") mlflow.autolog() print(\"reading the data...\") url = \"https://raw.githubusercontent.com/amitkaps/mlops/main/datasets/palmerpenguins/penguins.csv\" df = pd.read_csv(url) print(\"preparing the data...\") df = df.dropna() columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g'] X = df[columns] y = df.species print(\"splitting the data into train and test...\") X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) with mlflow.start_run(): print(\"training the model...\") params = dict(random_state=44, max_depth=6) model = DecisionTreeClassifier(**params) model.fit(X_train, y_train) mlflow.sklearn.log_model( sk_model=model, artifact_path=\"model\", registered_model_name=\"penguins-classfier\" ) print(\"validating...\") y_pred = model.predict(X_test) test_accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred) print(\"Accuracy on test data:\", test_accuracy)","title":"Training a scikit-learn model"},{"location":"pages/","text":"Practical ML Operations (MLOps) {% callout type=\"note\" %} This is work in progress. {% /callout %} Scope & Session Plan Day 1: MLOps Landscape and Building Reproducible Prototype Day 2: Continuous Training - Data, Model, Experiments & Testing Day 3: Continuous Deployment - Serving, Inference, Feedback & System Performance Session 1: Introduction & Concepts Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure Session 2: Systems & Operations Current Landscape and Maturity Level for ML Ops Define ML Ops: Requirements, Considerations, Interfaces, Data & Model Understand Process, Skills, and Tooling to move across Maturity Levels View Case Studies and Best Practices of ML Ops from the Industry Session 3: Prototype & Modularize Build a basic task-focused ML model for the case problem Create, build and deploy a local serving prototype ML system Modularize the Prototype Code, Data and Model for improved iteration Understand Trade-offs and Benefits of Modularized Approach Session 4: Versioning & Components Versioning in ML Ops: Workflow & Components Declarative Code, Data & Model Design Patterns Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts Create a Versioned System for the Case Example Session 5: Organising & Preparation Manage Data in ML Ops: Organize, Prepare & Reproduce Training Data Prep: Extraction, Sampling, Splitting & Windowing Component Setup: Labelling, Preprocessing, & Augmentation Modularise & isolate components for data preparation Session 6: Experiments & Training Baselines: Heuristic-driven, Simple ML baselines Setup for Rapid Experiments: Repeatable and Reproducible Experiment Tracking: Model Parameters, Optimisation Organise & Package for Data, Compute, and Infrastructure needs Session 7: Evaluation & Metrics Coarse-grained vs Fine-grained Model Evaluation Evaluation for Robustness: Stratification & Slicing, Confidence Metrics vs. Business Objectives & Interpretations Tracking evaluation metrics for coverage and failure cases Session 8: Testing & Validation Code Testing: Unit, Integration, System & Acceptance Data Testing: Schema, Expectations, Quality, Distribution Skew Model Testing: Training, Evaluation, Inference & Deployment Behavioural Testing: Invariance, Functionality, Directionality Session 9: Serving & Inference Align Objectives and Interfaces for Serving e.g Latency Inference Optimisation: Accelerators, Model Quantisation / Distillation Design for Production: Packages, Executables and Artefacts Thinking in Stages: Test, Canary, Development, Production Session 10: Monitoring & Feedback Performance monitoring and Post-Hoc Evaluation Predictive monitoring: Delayed Outcomes, Importance Weighting Measuring & Locating Drift: Data (Label & Feature), Concept Feedback loop: Integrating with Application Logging and Monitoring Session 11: Levels & Practice Advanced Levels of Maturity for MLOps Dataset Management: Catalogs, Quality, Lineage Feature Stores and Pipeline Orchestration Continuous Monitoring and Automated Triggering Session 12: Best Practices Advanced Case Study of a Successful MLOps Review of Best Practices for MLOps in Production Moving towards Continual Learning Data-centric Systems Learning Path and Way Forward","title":"**Practical ML Operations (MLOps)**"},{"location":"pages/#practical-ml-operations-mlops","text":"{% callout type=\"note\" %} This is work in progress. {% /callout %}","title":"Practical ML Operations (MLOps)"},{"location":"pages/#scope-session-plan","text":"Day 1: MLOps Landscape and Building Reproducible Prototype Day 2: Continuous Training - Data, Model, Experiments & Testing Day 3: Continuous Deployment - Serving, Inference, Feedback & System Performance","title":"Scope &amp; Session Plan"},{"location":"pages/#session-1-introduction-concepts","text":"Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure","title":"Session 1: Introduction &amp; Concepts"},{"location":"pages/#session-2-systems-operations","text":"Current Landscape and Maturity Level for ML Ops Define ML Ops: Requirements, Considerations, Interfaces, Data & Model Understand Process, Skills, and Tooling to move across Maturity Levels View Case Studies and Best Practices of ML Ops from the Industry","title":"Session 2: Systems &amp; Operations"},{"location":"pages/#session-3-prototype-modularize","text":"Build a basic task-focused ML model for the case problem Create, build and deploy a local serving prototype ML system Modularize the Prototype Code, Data and Model for improved iteration Understand Trade-offs and Benefits of Modularized Approach","title":"Session 3: Prototype &amp; Modularize"},{"location":"pages/#session-4-versioning-components","text":"Versioning in ML Ops: Workflow & Components Declarative Code, Data & Model Design Patterns Versioning: ML Code, Model Envelopes, Data Snapshots & Artefacts Create a Versioned System for the Case Example","title":"Session 4: Versioning &amp; Components"},{"location":"pages/#session-5-organising-preparation","text":"Manage Data in ML Ops: Organize, Prepare & Reproduce Training Data Prep: Extraction, Sampling, Splitting & Windowing Component Setup: Labelling, Preprocessing, & Augmentation Modularise & isolate components for data preparation","title":"Session 5: Organising &amp; Preparation"},{"location":"pages/#session-6-experiments-training","text":"Baselines: Heuristic-driven, Simple ML baselines Setup for Rapid Experiments: Repeatable and Reproducible Experiment Tracking: Model Parameters, Optimisation Organise & Package for Data, Compute, and Infrastructure needs","title":"Session 6: Experiments &amp; Training"},{"location":"pages/#session-7-evaluation-metrics","text":"Coarse-grained vs Fine-grained Model Evaluation Evaluation for Robustness: Stratification & Slicing, Confidence Metrics vs. Business Objectives & Interpretations Tracking evaluation metrics for coverage and failure cases","title":"Session 7: Evaluation &amp; Metrics"},{"location":"pages/#session-8-testing-validation","text":"Code Testing: Unit, Integration, System & Acceptance Data Testing: Schema, Expectations, Quality, Distribution Skew Model Testing: Training, Evaluation, Inference & Deployment Behavioural Testing: Invariance, Functionality, Directionality","title":"Session 8: Testing &amp; Validation"},{"location":"pages/#session-9-serving-inference","text":"Align Objectives and Interfaces for Serving e.g Latency Inference Optimisation: Accelerators, Model Quantisation / Distillation Design for Production: Packages, Executables and Artefacts Thinking in Stages: Test, Canary, Development, Production","title":"Session 9: Serving &amp; Inference"},{"location":"pages/#session-10-monitoring-feedback","text":"Performance monitoring and Post-Hoc Evaluation Predictive monitoring: Delayed Outcomes, Importance Weighting Measuring & Locating Drift: Data (Label & Feature), Concept Feedback loop: Integrating with Application Logging and Monitoring","title":"Session 10: Monitoring &amp; Feedback"},{"location":"pages/#session-11-levels-practice","text":"Advanced Levels of Maturity for MLOps Dataset Management: Catalogs, Quality, Lineage Feature Stores and Pipeline Orchestration Continuous Monitoring and Automated Triggering","title":"Session 11: Levels &amp; Practice"},{"location":"pages/#session-12-best-practices","text":"Advanced Case Study of a Successful MLOps Review of Best Practices for MLOps in Production Moving towards Continual Learning Data-centric Systems Learning Path and Way Forward","title":"Session 12: Best Practices"},{"location":"pages/introduction-concepts/","text":"Introduction & Concepts Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure Intro Instructor Introduction Participant Introduction Experience ML Experience Personal ML Workflow Org ML Workflow / Infra / Setup for ML Right now Sample Project in Production (2 - 3 Examples) Type: ML / DL / Forecasting Process Data Storage / ELT Compute Abstraction (Laptop / Cloud) Productionion WorkFlow Monitoring & ReTraining Grouping People with Diverse Background Course Expecation Expectation Not focussed on ML/DL model building Framework, System Agnostics What is the Focus? Difference between PoC and Production ML System Building the Model (Algorithm) is easy? Solving real world problems Success of ML Projects in Production is low? Software Engineering vs. Programming -> Change over Time Objectives Compare: Project vs. Product/System Approach Comparing: Exploration vs. Production in ML Objective | Model Performance | Business Performance Computation Priority | Fast Training | Fast Inference, low latency Data | Clean / Static / Historical | Messy / Dynamic / + Streaming / Changing / Privacy Fairness / Interepretability | Maybe | Important Comparing: Software Systems vs. ML Systems Diagram for Software Systems Code - Modular Design Data - Separate Tests (Unit Test, Integration Test) Build (Package) Deployment (Package) Deploying Reliably is hard (Case Example) Deploy Multiple times at a times (DevOps Evolution) Speed of delivery Evolution of Building Software Evolution over years: Shift from separate Dev QA, SysAdmin -> DevOps Role -> CI/CD 1 deploy a month to 100s of deploy per day UI Developments Model Update - DevOps vs Normal What is different Team Skills: Roles Development Testing Deployment Production Requirements of an ML Systems Reliable: Correct function at desired level of performance and failure resistance Fail Silently Scalability: Scale Up and Down: Data Volume, Traffic Volume and Complexity Maintainbility: People / Skills - Hetrogenous Tools & Background Adaptibility: Data Distribution and Business Requirement - React Quickly Exercise Real Life Example (Take Three - Select One) Structured Data Simple Problem (not multi-objective) Inference at Real-Time with Considerations: Framing, Objectives, Constraints & Phases Framing: Right Problem for the ML Objection Example: MultiClass Decision Boundary (Which App to open) Objective: ML Objective: Performance, Latency Business Objective: Cost, ROI, Regulation Baseline, Threshold, False Negative, Interpretability, Confidence Constrain: Time & Budget (More Powerful Machine, More Experiment), Privacy Phased: Heuristic, Simple, ML, Complex Thinking in Systems: Application, Business and Infrastructure Link to the Exercise","title":"Introduction & Concepts"},{"location":"pages/introduction-concepts/#introduction-concepts","text":"Challenges for ML driven Operations in Production Requirements: Reliable, Scalable, Maintainable, Adaptable Considerations: Framing, Objectives, Constraints & Phases Thinking in Systems: Application, Business and Infrastructure","title":"Introduction &amp; Concepts"},{"location":"pages/introduction-concepts/#intro","text":"Instructor Introduction Participant Introduction Experience ML Experience Personal ML Workflow Org ML Workflow / Infra / Setup for ML Right now Sample Project in Production (2 - 3 Examples) Type: ML / DL / Forecasting Process Data Storage / ELT Compute Abstraction (Laptop / Cloud) Productionion WorkFlow Monitoring & ReTraining Grouping People with Diverse Background","title":"Intro"},{"location":"pages/introduction-concepts/#course-expecation","text":"Expectation Not focussed on ML/DL model building Framework, System Agnostics","title":"Course Expecation"},{"location":"pages/introduction-concepts/#what-is-the-focus","text":"Difference between PoC and Production ML System Building the Model (Algorithm) is easy? Solving real world problems Success of ML Projects in Production is low? Software Engineering vs. Programming -> Change over Time","title":"What is the Focus?"},{"location":"pages/introduction-concepts/#objectives","text":"","title":"Objectives"},{"location":"pages/introduction-concepts/#compare-project-vs-productsystem-approach","text":"","title":"Compare: Project vs. Product/System Approach"},{"location":"pages/introduction-concepts/#comparing-exploration-vs-production-in-ml","text":"Objective | Model Performance | Business Performance Computation Priority | Fast Training | Fast Inference, low latency Data | Clean / Static / Historical | Messy / Dynamic / + Streaming / Changing / Privacy Fairness / Interepretability | Maybe | Important","title":"Comparing: Exploration vs. Production in ML"},{"location":"pages/introduction-concepts/#comparing-software-systems-vs-ml-systems","text":"Diagram for Software Systems Code - Modular Design Data - Separate Tests (Unit Test, Integration Test) Build (Package) Deployment (Package) Deploying Reliably is hard (Case Example) Deploy Multiple times at a times (DevOps Evolution) Speed of delivery","title":"Comparing: Software Systems vs. ML Systems"},{"location":"pages/introduction-concepts/#evolution-of-building-software","text":"Evolution over years: Shift from separate Dev QA, SysAdmin -> DevOps Role -> CI/CD 1 deploy a month to 100s of deploy per day UI Developments Model Update - DevOps vs Normal What is different Team Skills: Roles Development Testing Deployment Production","title":"Evolution of Building Software"},{"location":"pages/introduction-concepts/#requirements-of-an-ml-systems","text":"Reliable: Correct function at desired level of performance and failure resistance Fail Silently Scalability: Scale Up and Down: Data Volume, Traffic Volume and Complexity Maintainbility: People / Skills - Hetrogenous Tools & Background Adaptibility: Data Distribution and Business Requirement - React Quickly","title":"Requirements of an ML Systems"},{"location":"pages/introduction-concepts/#exercise","text":"Real Life Example (Take Three - Select One) Structured Data Simple Problem (not multi-objective) Inference at Real-Time with","title":"Exercise"},{"location":"pages/introduction-concepts/#considerations-framing-objectives-constraints-phases","text":"Framing: Right Problem for the ML Objection Example: MultiClass Decision Boundary (Which App to open) Objective: ML Objective: Performance, Latency Business Objective: Cost, ROI, Regulation Baseline, Threshold, False Negative, Interpretability, Confidence Constrain: Time & Budget (More Powerful Machine, More Experiment), Privacy Phased: Heuristic, Simple, ML, Complex","title":"Considerations: Framing, Objectives, Constraints &amp; Phases"},{"location":"pages/introduction-concepts/#thinking-in-systems-application-business-and-infrastructure","text":"Link to the Exercise","title":"Thinking in Systems: Application, Business and Infrastructure"},{"location":"pages/learning-obectives/","text":"Learning Objectives Remember -> Recall / Retrieve / Reproduce Understand -> Explain / Demonstrate / Discuss Apply -> Use / Solve Analyze -> Examine / Break Evaluate -> Defend / Justify Create -> Generate / Compile","title":"Learning Objectives"},{"location":"pages/learning-obectives/#learning-objectives","text":"Remember -> Recall / Retrieve / Reproduce Understand -> Explain / Demonstrate / Discuss Apply -> Use / Solve Analyze -> Examine / Break Evaluate -> Defend / Justify Create -> Generate / Compile","title":"Learning Objectives"},{"location":"pages/levels/","text":"Levels Maturity Map Level 1, 2 ,3, 4, 5 Feature Engineering Pipeline Data Ingestion Data Loading Data Preloading Data Experimentation Data Analysis Data Preparation & Validation Model Training Model Validation Model Packaging Automated Ml Pipeline","title":"Levels Maturity Map"},{"location":"pages/levels/#levels-maturity-map","text":"Level 1, 2 ,3, 4, 5 Feature Engineering Pipeline Data Ingestion Data Loading Data Preloading Data Experimentation Data Analysis Data Preparation & Validation Model Training Model Validation Model Packaging Automated Ml Pipeline","title":"Levels Maturity Map"},{"location":"pages/systems-operations/","text":"Systems & Operations Levels of Maturity Benifits & Challenges at each level Level 1: Manual, Notebook driven (Visual) Notebook for Data Preparation, Model Traning, Model Validation Script for Serving Example: Classification Penguin Level 2: Reproducible ML Project Structure (Convention / Practices) All Code in Version All Data Source & Model are Tagged Versioning Code Level 3: Modular ML Data Validation, Model Validation Modular Steps Unit Testing Experiment Tracking Model Registry Data Catalog Level 4: Automated ML Code and Pipeline are Version Control Compute Platform (Orchestration) Modular Pipeline Experiment Tracking Data Lineage Feature Store (PreCompute and Metadata) Model Repository Serving Platform - Auto deploy Monitoriing & Alerting","title":"Systems & Operations"},{"location":"pages/systems-operations/#systems-operations","text":"","title":"Systems &amp; Operations"},{"location":"pages/systems-operations/#levels-of-maturity","text":"Benifits & Challenges at each level Level 1: Manual, Notebook driven (Visual) Notebook for Data Preparation, Model Traning, Model Validation Script for Serving Example: Classification Penguin Level 2: Reproducible ML Project Structure (Convention / Practices) All Code in Version All Data Source & Model are Tagged Versioning Code Level 3: Modular ML Data Validation, Model Validation Modular Steps Unit Testing Experiment Tracking Model Registry Data Catalog Level 4: Automated ML Code and Pipeline are Version Control Compute Platform (Orchestration) Modular Pipeline Experiment Tracking Data Lineage Feature Store (PreCompute and Metadata) Model Repository Serving Platform - Auto deploy Monitoriing & Alerting","title":"Levels of Maturity"}]}